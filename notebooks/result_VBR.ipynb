{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e591558",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71333a17-bb4f-4143-bd04-5f75aea37cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openhexa.sdk import workspace\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import os\n",
    "from os import listdir, environ\n",
    "from copy import deepcopy\n",
    "from os.path import isfile, join\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "594b444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "local = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a3eff",
   "metadata": {},
   "source": [
    "# 2. Creating the statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1556d9",
   "metadata": {},
   "source": [
    "## 2.1 Define the necessary parameters\n",
    "\n",
    "Variable name changes:\n",
    "\n",
    "**Folder names**\n",
    "- result_simulation --> simulation_statistics\n",
    "- Selections_Verif --> verification_information\n",
    "\n",
    "**Parameters in the file names**\n",
    "(These I changed because it was complaning about the file names being too long,\n",
    "not because I find them clearer).\n",
    "- GAIN_VERIF_MEDIAN_MAX --> gain_verif\n",
    "- MIN_NB_TRIM_OBS --> obs_win\n",
    "- MIN_NB_TRIM_WITH_VERIF --> min_nb_verif\n",
    "- seuil_max_bas_risk --> seuil_b\n",
    "- seuil_max_moyen_risk --> seuil_m\n",
    "- Paiement --> pai\n",
    "- Quality_risk --> qual_risk\n",
    "\n",
    "**Column names in Selections_Verif / verification_information**\n",
    "- verified --> bool_verified\n",
    "\n",
    "**Column names in result_simulation / simulation_statistics**\n",
    "- cout total (VBR) --> total cost (VBR)\n",
    "- cout total (syst) --> total cost (syst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16d87f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local:\n",
    "    data_path = os.path.join(os.getcwd(), \"data\")\n",
    "else:\n",
    "    data_path = f\"{workspace.files_path}/PBF burundi extraction/data\"\n",
    "\n",
    "DB_names = {\n",
    "    os.path.join(data_path, \"verification_information\"): \"VBR_liste_detaillees\",\n",
    "    os.path.join(data_path, \"simulation_statistics\"): \"VBR_results\",\n",
    "}\n",
    "\n",
    "dict_params = {\n",
    "    \"gain_verif\": [],\n",
    "    \"obs_win\": [],\n",
    "    \"min_nb_verif\": [],\n",
    "    \"p_low\": [],\n",
    "    \"p_mod\": [],\n",
    "    \"p_high\": [],\n",
    "    \"cout_verif\": [],\n",
    "    \"seuil_b\": [],\n",
    "    \"seuil_m\": [],\n",
    "    \"pai\": [],\n",
    "    \"qual_risk\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cc4321",
   "metadata": {},
   "source": [
    "## 2.2 Define the necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0f92f0-8ed0-4d91-ba3c-a400ebc0581d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_simulation_name(filename):\n",
    "    return all(substring in filename for substring, _ in dict_params.items())\n",
    "\n",
    "\n",
    "def get_parameters(f):\n",
    "    dict_params_full = deepcopy(dict_params)\n",
    "    dict_params_full[\"name\"] = f\n",
    "    dict_params_full[\"model\"] = [\"defaut\"]\n",
    "    list_file_params = f.split(\"-\")\n",
    "    for p in dict_params:\n",
    "        for p_match in list_file_params:\n",
    "            if p in p_match:\n",
    "                dict_params_full.setdefault(p, []).append(\n",
    "                    p_match.split(\"___\")[1].replace(\".csv\", \"\")\n",
    "                )\n",
    "    return pd.DataFrame.from_dict(dict_params_full)\n",
    "\n",
    "\n",
    "def get_statistics(mypath, f):\n",
    "    df = pd.read_csv(join(mypath, f))\n",
    "    df[\"name\"] = f\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_verification_info(df):\n",
    "    df[\"nb_centers_verified\"] = df[\"bool_verified\"].map(lambda x: 1 if x else 0)\n",
    "    df[\"nb_centers\"] = 1\n",
    "\n",
    "    df[\"#_scores_risque_eleve\"] = df[\"categorie_risque\"].map(\n",
    "        lambda x: 1 if x == \"high\" or x == \"uneligible\" else 0\n",
    "    )\n",
    "    df[\"#_scores_risque_mod1\"] = df[\"categorie_risque\"].map(lambda x: 1 if x == \"moderate_1\" else 0)\n",
    "    df[\"#_scores_risque_mod2\"] = df[\"categorie_risque\"].map(lambda x: 1 if x == \"moderate_2\" else 0)\n",
    "    df[\"#_scores_risque_mod3\"] = df[\"categorie_risque\"].map(lambda x: 1 if x == \"moderate_3\" else 0)\n",
    "    df[\"#_scores_risque_mod\"] = df[\"categorie_risque\"].map(lambda x: 1 if x == \"moderate\" else 0)\n",
    "    df[\"#_scores_risque_faible\"] = df[\"categorie_risque\"].map(lambda x: 1 if x == \"low\" else 0)\n",
    "    # These moderate_1, moderate_2, etc should probably be in a config file instead of hard coded here...\n",
    "    df = df.groupby(\n",
    "        [\n",
    "            \"period\",\n",
    "            \"model\",\n",
    "            \"p_high\",\n",
    "            \"p_mod\",\n",
    "            \"p_low\",\n",
    "            \"min_nb_verif\",\n",
    "            \"obs_win\",\n",
    "            \"gain_verif\",\n",
    "            \"pai\",\n",
    "            \"name\",\n",
    "            \"level_2_name\",\n",
    "            \"level_3_name\",\n",
    "        ],\n",
    "        as_index=False,\n",
    "    )[\n",
    "        [\n",
    "            \"nb_centers\",\n",
    "            \"nb_centers_verified\",\n",
    "            \"#_scores_risque_faible\",\n",
    "            \"#_scores_risque_mod1\",\n",
    "            \"#_scores_risque_mod2\",\n",
    "            \"#_scores_risque_mod3\",\n",
    "            \"#_scores_risque_eleve\",\n",
    "            \"#_scores_risque_mod\",\n",
    "        ]\n",
    "    ].sum()\n",
    "    return df\n",
    "\n",
    "\n",
    "def str_to_date(datestr):\n",
    "    if isinstance(datestr, str) and \"Q\" in datestr:\n",
    "        return datestr[:4] + \"-\" + str(int(datestr[5]) * 3) + \"-1\"\n",
    "    elif isinstance(datestr, int) or datestr.isdigit():\n",
    "        x = int(datestr)\n",
    "        return f\"{x // 100}-{x % 100}-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c769c209",
   "metadata": {},
   "source": [
    "## 2.3 Create the output csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633d37d8-b950-4ccb-87aa-c5e42cabce95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For the VBR_liste_detaillees, the columns are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['periode', 'model', 'p_high', 'p_mod', 'p_low', 'min_nb_verif',\n",
       "       'obs_win', 'gain_verif', 'pai', 'province', 'level_3_name',\n",
       "       'nb_centers', 'nb_centers_verified', '#_scores_risque_faible',\n",
       "       '#_scores_risque_mod1', '#_scores_risque_mod2',\n",
       "       '#_scores_risque_mod3', '#_scores_risque_eleve',\n",
       "       '#_scores_risque_mod'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'For the VBR_liste_verification, the columns are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['gain_verif', 'obs_win', 'min_nb_verif', 'p_low', 'p_mod',\n",
       "       'p_high', 'cout_verif', 'seuil_b', 'seuil_m', 'pai', 'qual_risk',\n",
       "       'model', 'periode', 'ou_id', 'level_2_uid', 'province',\n",
       "       'level_3_uid', 'level_3_name', 'level_4_uid', 'level_4_name',\n",
       "       'level_5_uid', 'level_5_name', 'level_6_uid', 'level_6_name',\n",
       "       'bool_verified', 'diff_in_subsidies_decval_period',\n",
       "       'diff_in_subsidies_tauxval_period', 'benefice_complet_vbr',\n",
       "       'taux_validation', 'subside_dec_period', 'subside_val_period',\n",
       "       'subside_taux_period', 'ecart_median', 'categorie_risque', 'date'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'For the VBR_results, the columns are:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['gain_verif', 'obs_win', 'min_nb_verif', 'p_low', 'p_mod',\n",
       "       'p_high', 'cout_verif', 'seuil_b', 'seuil_m', 'pai', 'qual_risk',\n",
       "       'model', 'province', 'periode', 'total number of centres',\n",
       "       'number of centers high risk', 'number of centers middle risk',\n",
       "       'number of centers low risk', 'number of verified centers',\n",
       "       'cost of verification (VBR)', 'cost of verification (syst)',\n",
       "       'subsidies (VBR)', 'subsidies (syst)', 'total cost (VBR)',\n",
       "       'total cost (syst)', 'ratio cost_verification cost_total (VBR)',\n",
       "       'ratio cost_verification cost_total (syst)',\n",
       "       'Number of centers where vbr was beneficial',\n",
       "       'Number of centers over-subsidized',\n",
       "       'Money saved by VBR (taking into account verif costs)',\n",
       "       'Amount of over-subsidies',\n",
       "       'Average of over-subsidies (non-verified centers)',\n",
       "       'Average of over-subsidies (verified centers)', 'gain_vbr'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mypath, _ in DB_names.items():\n",
    "    # initialize the variables\n",
    "    dfs = pd.DataFrame()\n",
    "    dfs_detailled = pd.DataFrame()\n",
    "    list_files = [\n",
    "        f for f in listdir(mypath) if isfile(join(mypath, f)) and valid_simulation_name(f)\n",
    "    ]\n",
    "\n",
    "    # Get the files in the path\n",
    "    for filename in list_files:\n",
    "        df = get_parameters(filename).merge(get_statistics(mypath, filename), on=\"name\")\n",
    "\n",
    "        if \"verification_information\" in mypath:\n",
    "            dfs_detailled = pd.concat([dfs_detailled, df], ignore_index=True)\n",
    "            df = process_verification_info(df)\n",
    "            \n",
    "        dfs = pd.concat([dfs, df], ignore_index=True)\n",
    "\n",
    "    # Clean and save the csv that concatenates the information in the input files. \n",
    "    dfs.rename(columns={\"level_2_name\": \"province\", \"period\": \"periode\"}, inplace=True)\n",
    "    dfs = dfs.sort_values([\"province\", \"periode\"]).drop(\"name\", axis=1)\n",
    "    if \"simulation_statistics\" in mypath:\n",
    "        dfs[\"gain_vbr\"] = dfs[\"total cost (syst)\"] - dfs[\"total cost (VBR)\"]\n",
    "    if not local:\n",
    "        engine = create_engine(environ[\"WORKSPACE_DATABASE_URL\"])\n",
    "        dfs.to_sql(DB_names[mypath], con=engine, if_exists=\"replace\")\n",
    "    dfs.to_csv(f\"{data_path}/{DB_names[mypath]}.csv\", index=False)\n",
    "    display(f\"For the {DB_names[mypath]}, the columns are:\")\n",
    "    display(dfs.columns.values)\n",
    "\n",
    "    # For the verification_information file, we also save the information in a detailed way. \n",
    "    if \"verification_information\" in mypath:\n",
    "        dfs_detailled[\"bool_verified\"] = dfs_detailled[\"bool_verified\"].astype(int)\n",
    "        dfs_detailled.rename(\n",
    "            columns={\"level_2_name\": \"province\", \"period\": \"periode\"}, inplace=True\n",
    "        )\n",
    "        dfs_detailled[\"date\"] = dfs_detailled[\"periode\"].map(str_to_date)\n",
    "        dfs_detailled = dfs_detailled.sort_values([\"province\", \"periode\"]).drop(\"name\", axis=1)\n",
    "        dfs_detailled.to_csv(f\"{data_path}/VBR_liste_verification.csv\", index=False)\n",
    "        if not local:\n",
    "            dfs_detailled.to_sql(\"VBR_liste_verification\", con=engine, if_exists=\"replace\")\n",
    "        display(\"For the VBR_liste_verification, the columns are:\")\n",
    "        display(dfs_detailled.columns.values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vbr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
